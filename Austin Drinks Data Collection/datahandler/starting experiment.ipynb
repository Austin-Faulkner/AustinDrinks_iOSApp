{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"starting experiment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mPguXOIxkHAb"},"source":["**To Do:**\n","\n","- REFACTOR, Clean up code considerably!\n","  -  separate csv and json saving methods.\n"]},{"cell_type":"code","metadata":{"id":"ukUoRkSchOEK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641773441543,"user_tz":360,"elapsed":8932,"user":{"displayName":"Austin Faulkner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiL4U4eSCdMz9tJIdaqafSrskkrgehAQ-ugtFxu-w=s64","userId":"13684566176525076719"}},"outputId":"c5dc1d60-cf64-406e-f33c-b3c8b48668d2"},"source":["!pip install selenium # in package file"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 122 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 194 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 204 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 215 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 225 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 235 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 245 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 256 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 266 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 286 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 307 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 317 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 337 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 348 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 368 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 378 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 389 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 399 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 409 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 419 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 430 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 440 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 450 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 460 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 471 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 481 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 491 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 501 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 512 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 522 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 532 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 542 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 552 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 563 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 573 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 583 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 593 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 604 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 614 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 624 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 634 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 645 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 655 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 665 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 675 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 686 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 696 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 706 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 716 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 727 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 737 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 747 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 757 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 768 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 778 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 788 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 798 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 808 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 819 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 829 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 839 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 849 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 860 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 870 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 880 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 890 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 901 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 911 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 921 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 931 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 942 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 952 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 958 kB 14.8 MB/s \n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Collecting urllib3[secure]~=1.26\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 22.6 MB/s \n","\u001b[?25hCollecting trio~=0.17\n","  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n","\u001b[K     |████████████████████████████████| 356 kB 45.8 MB/s \n","\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n","Collecting outcome\n","  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n","Collecting sniffio\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n","Collecting cryptography>=1.3.4\n","  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 38.1 MB/s \n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n","Collecting pyOpenSSL>=0.14\n","  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s \n","\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.0.0\n"]}]},{"cell_type":"code","metadata":{"id":"DBJBOhSSxv3W"},"source":["#imports\n","from bs4 import BeautifulSoup\n","from datetime import datetime as dt\n","from google.colab import drive\n","from selenium.webdriver.support.ui import WebDriverWait       \n","from selenium.webdriver.common.by import By       \n","from selenium.webdriver.support import expected_conditions as EC\n","from urllib import request\n","from urllib.request import urlopen\n","import csv\n","import io\n","import json\n","import os\n","import random\n","import re\n","import time   # to slow down the time between requests\n","              # to minimize the risk of ip addresses being blocked."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4fo1UbviKyQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641773472233,"user_tz":360,"elapsed":30528,"user":{"displayName":"Austin Faulkner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiL4U4eSCdMz9tJIdaqafSrskkrgehAQ-ugtFxu-w=s64","userId":"13684566176525076719"}},"outputId":"9174b256-7206-46d9-cec9-252dcfa7fed6"},"source":["\n","!apt-get update\n","!apt install chromium-chromedriver\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Ign:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:8 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,489 kB]\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Get:10 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n","Get:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [716 kB]\n","Get:13 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n","Ign:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:16 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n","Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [749 kB]\n","Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,929 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.5 kB]\n","Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Get:27 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.7 kB]\n","Fetched 13.7 MB in 4s (3,765 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n","Suggested packages:\n","  webaccounts-chromium-extension unity-chromium-extension\n","The following NEW packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-chromedriver\n","  chromium-codecs-ffmpeg-extra\n","0 upgraded, 4 newly installed, 0 to remove and 61 not upgraded.\n","Need to get 94.0 MB of archives.\n","After this operation, 324 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 95.0.4638.69-0ubuntu0.18.04.1 [1,135 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 95.0.4638.69-0ubuntu0.18.04.1 [83.6 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 95.0.4638.69-0ubuntu0.18.04.1 [4,249 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 95.0.4638.69-0ubuntu0.18.04.1 [4,986 kB]\n","Fetched 94.0 MB in 3s (28.0 MB/s)\n","Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n","(Reading database ... 155225 files and directories currently installed.)\n","Preparing to unpack .../chromium-codecs-ffmpeg-extra_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser.\n","Preparing to unpack .../chromium-browser_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser-l10n.\n","Preparing to unpack .../chromium-browser-l10n_95.0.4638.69-0ubuntu0.18.04.1_all.deb ...\n","Unpacking chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Setting up chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1Xcktu_P6nz","executionInfo":{"status":"ok","timestamp":1641773500324,"user_tz":360,"elapsed":28101,"user":{"displayName":"Austin Faulkner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiL4U4eSCdMz9tJIdaqafSrskkrgehAQ-ugtFxu-w=s64","userId":"13684566176525076719"}},"outputId":"e1d28f6b-f39f-49b8-a7e4-10bc2b04d1cb"},"source":["drive.mount('/content/gdrive')\n","ROOT = 'gdrive/MyDrive/Austin_Drinks'\n","ROOT_IMPORTS = 'gdrive/MyDrive/Colab Notebooks/Austin_Drinks_Package';"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"_1GqVSeNdpcg"},"source":["SITES is a dictionary containing attributes such as url, attributes, attribute definitions, and search extensions\n","\n","If these items on the website changes, then it will be a simple update in the SITES dictionary."]},{"cell_type":"code","metadata":{"id":"bZ7n2yKPHgiF"},"source":["SITES =   { \n","            'yelp' :  {\n","                        # base url for yelp\n","                        'url':'https://www.yelp.com',\n","                        # header containing the anchor tag we need for url's extention to each establishment page\n","                        'search_results_tags' : 'h4',\n","                       \n","                        # search_results_tags \n","                        # this class is associated with distilleries, breweries and wineries search results\n","                        # thus, this class should work for all establishment types.\n","                        # the h4 class containing the anchor tags we need for url's extension to each establishment page\n","                        'search_results_tags_class' : 'css-1l5lt1i',\n","                       \n","                        # attributes of button to reveal accommodations and unaccommodations\n","                        'amenities_reveal_button_text' : 'Attribute',\n","                        'amenities_reveal_button_attr' : 'aria-controls',\n","                        'amenities_reveal_button_attr2' : 'class',\n","                        'amenities_reveal_button_attr2_def' : ' css-174jypu',\n","                       \n","                       # tag information to get the json data\n","                        'json_data_tag' : 'div',\n","                        'json_data_tag_attr' : 'class',\n","                        'json_data_tag_attr_def' : 'main-content-wrap main-content-wrap--full',\n","                       \n","                       # tag information to get businesses url\n","                        'business_url_tag' : 'div',\n","                        'business_url_tag_attr' : 'class',\n","                        'business_url_tag_attr_def' : ' css-1vhakgw border--top__373c0__2SDyx border-color--default__373c0__1WSID',\n","                       \n","                       # tag information to get establishment's tags\n","                        'est_tags_tag' : 'span',\n","                        'est_tags_tag_attr' : 'class',\n","                        'est_tags_tag_attr_def' : ' display--inline__373c0__3d-lf margin-r1__373c0__7ZINV border-color--default__373c0__2s5dW',\n","                        'est_tags_sub_tag_attr' : 'a',\n","                       \n","                       # hours of operation table tag information\n","                        'hoO_tag' : 'table',\n","                        'hoO_tag_attr' : 'class',\n","                        'hoO_tag_attr_def' : ' hours-table__373c0__2YHlD table__373c0__1FIZ8 table--simple__373c0__3QsR_',\n","                        'hoO_row_tag_attr' : 'class',\n","                        'hoO_row_tag_attr_def' : ' table-row__373c0__1F6B0',\n","                       \n","                       # amenities and anti-amenities tag information\n","                        'amenities_tag' : 'span', \n","                        'amenities_tag_attr' : 'class', \n","                        'amenities_tag_attr_def' : ' css-1h1j0y3', \n","                        'anti_amenities_tag' : 'span', \n","                        'anti_amenities_tag_attr' : 'class', \n","                        'anti_amenities_tag_attr_def' : ' css-1h1j0y3', \n","                       \n","                        #different extensions to be used for yelp.com.\n","                        'austin_distillery_search_ext': '/search?find_desc=distilleries&find_loc=Austin%2C+TX',\n","                        'austin_winery_search_ext' : '/search?find_desc=wineries&find_loc=Austin%2C%20TX',\n","                        'austin_brewery_search_ext' : '/search?find_desc=breweries&find_loc=Austin%2C%20TX',\n","                       #add methods to save locations, use datetime to track when information needs to be updated (think permanently closed establishments)\n","                      }\n","           }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4IMqkrTK8vq"},"source":["Notes/References on class creation"]},{"cell_type":"code","metadata":{"id":"aR2pnSm1gEUf"},"source":["# using type function to create classes on the fly.\n","\n","# est_obj = type('est_class', (object,), { 'variable_1': value_1, 'variable_2': value_2, ... , 'variable_n': value_n })()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBgffWMchPFA"},"source":["# or use a class definition\n","\n","# class est_class():\n","#   def __init__(self, value_1, value_2, ... , value_n):\n","#     self.variable_1 = value_1\n","#     self.variable_2 = value_2\n","#     ...\n","#     self.variable_n = value_n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61S9vH6TdrVg"},"source":["Used to log errors that occur during the scraping process"]},{"cell_type":"code","metadata":{"id":"j3GUDUEi1gmo"},"source":["def __errorLog(link, error):\n","  with open(ROOT + '/Data_Collecting/error_log.txt', 'a') as f:\n","    f.write('{}\\t\\t{}\\t\\t{}\\n'.format(dt.now(), link, error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkSkUezKv3TJ"},"source":["Utility functions to save and load JSON files"]},{"cell_type":"code","metadata":{"id":"kEN-mjjJz8_F"},"source":["# save JSON data\n","\n","def saveJSON(data):  \n","  with io.open(ROOT + '/Data_Collecting/JSON files/' + data['name'] + '.json', 'w', encoding='utf-8') as f:\n","    json.dump(data, f, ensure_ascii = False, indent = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JckuwP3tOjvj"},"source":["def loadJSON(name):  \n","  with io.open(ROOT + '/Data_Collecting/JSON files/' + name, 'r', encoding='utf-8') as f:\n","    data = json.load(f)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utility functions to save, append, and update CSV files."],"metadata":{"id":"PRdEEgMEgqXK"}},{"cell_type":"code","source":["def saveNewCSV(data):\n","  with io.open(ROOT + )"],"metadata":{"id":"CVzus8iIgqFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVs8Wrnm1Mpz"},"source":["Function to clean names up, replace unwanted symbols such as '&' and such."]},{"cell_type":"code","metadata":{"id":"FJXjyAFx1Krj"},"source":["def cleanName(name):\n","  cleaned = re.sub('&amp;', 'and', name)\n","  cleaned = re.sub('&apos;', '\\'', cleaned)\n","  return cleaned"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"td2pWJ9gGNOF"},"source":["Use urllib to get a url's html and return a beautifulsoup object"]},{"cell_type":"code","metadata":{"id":"Hlp42ZM1jiuU"},"source":["# function specifically designed to find and click a button.\n","\n","def getBS_via_selenium(url, onclickButton = False, in_text = '', attr = ''):\n","  chrome_options = Options()\n","  chrome_options.add_argument('--headless') # don't spawn window\n","  chrome_options.add_argument('--no-sandbox')\n","  chrome_options.add_argument('--disable-dev-shm-usage')\n","  chrome_options.add_argument('user-agent = Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36')\n","  driver = webdriver.Chrome(executable_path = '/usr/lib/chromium-browser/chromedriver', options = chrome_options)\n","  print('Compiling BS object at url @ ' + url)\n","\n","  # first, get the page where the button we want is at\n","  source = None\n","  attempts = 0\n","  max_attempts = 4\n","  unsuccessful = True \n","  while unsuccessful:\n","    attempts += 1\n","    try:\n","      print('Getting url via selenium')\n","      driver.get(url)\n","      time.sleep(random.randint(10,14))\n","    except BaseException as ex:\n","      if attempts < max_attempts:\n","        print('Unable to open url. Retrying in 30 seconds. \\\n","               See exception repr for details: ')\n","        print(repr(ex))      # print object as string\n","        time.sleep(30)\n","      else:\n","        print('Too many attepts, breaking loop')\n","        #log error to file.\n","        __errorLog(url, 'Too many attempts')\n","    else:\n","      unsuccessful = False\n","      source = BeautifulSoup(driver.page_source, 'html.parser')\n","\n","    \n","      # second, if we are interested in clicking a button,\n","      # we repeat the process using the existing driver to click the button\n","      # and extract the data\n","\n","      if onclickButton:\n","          attempts = 0\n","          max_attempts = 2\n","          unsuccessful = True \n","          while unsuccessful:\n","            attempts += 1\n","            try:\n","              # this aria-controls is unique to the button we are looking for..\n"," #             buttons = source.find_all('button', {'class': ' css-174jypu'})  # find buttons with class of interest\n","              attr_def = ''\n","              for button in source.find_all('button', {'class': ' css-174jypu'}):\n","                if in_text in button.text:\n","                  in_text_found = True\n","                  attr_def = button[attr]\n","\n","#                  <button id=\"onetrust-pc-btn-handler\" tabindex=\"0\"> is blocking my ability to click a button now, wtf..\n","                  driver.execute_script(\"arguments[0].click();\", WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[' + attr + '=' + attr_def + ']'))))\n","#                  WebDriverWait(driver, 20).until(EC.invisibility_of_element((By.CSS_SELECTOR, 'button[' + attr + '=' + attr_def + ']'))).click()\n","#                  driver.execute_script(\"arguments[0].click();\", WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.\" + attr_def))))\n","\n","#                  element = driver.find_element_by_css_selector('button[' + attr + '=' + attr_def + ']')\n","#                  element.click()\n","                  print('clicked element')\n","                  time.sleep(random.randint(3,6))\n","                  break\n","            except BaseException as ex:\n","              if attempts < max_attempts:\n","                print('Unable to click button. Retrying in 30 seconds. \\\n","                      See exception repr for details: ')\n","                print(repr(ex))      # print object as string\n","                time.sleep(30)\n","              else:\n","                print('Too many attempts to click button, breaking loop')\n","                #log error to file.\n","                __errorLog(url, 'Exceeded click button request limit to extract Amenities')\n","            else:\n","              unsuccessful = False             \n","              source = BeautifulSoup(driver.page_source, 'html.parser')              \n","\n","  driver.close()\n","  driver.quit()\n","  return source"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQlWnwdLGsGm"},"source":["# takes a url as input (must be a complete url as string) and returns a BS object.\n","\n","def getBS_urllib(url):\n","  req = request.Request(\n","      url,\n","      headers = {\n","          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'\n","          }\n","      )\n","  print('Compiling BS object at url @ ' + url)\n","  attempts = 0\n","  max_attempts = 4\n","  html = None\n","  bs = None\n","  unsuccessful = True \n","  while unsuccessful:\n","    attempts += 1\n","    try:\n","      html = urlopen(req)\n","      time.sleep(random.randint(3,8))\n","    except BaseException as ex:\n","      if attempts < max_attempts:\n","        print('Unable to open url. Retrying in 30 seconds.  See exception repr for details:')\n","        print(repr(ex))      # print object as string\n","        time.sleep(30)\n","      else:\n","        print('Too many attepts, breaking loop')\n","        #log error to file.\n","        __errorLog(url, 'Exceeded url request limit')\n","        break\n","    else:\n","      unsuccessful = False\n","      bs = BeautifulSoup(html.read(), 'html.parser')\n","  return bs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7T0Vdaf1-Khk"},"source":["Define a function that returns a list of search result links"]},{"cell_type":"code","metadata":{"id":"Pn2oiItOx6UW"},"source":["# currently, this may only work for yelp.com.\n","# trip advisor for sure is a different story.\n","\n","def get_yelp_search_result_links(url_curr, lst):\n","\n","  print('Gathering search result urls @ ' + url_curr)\n","\n","  bs = getBS_urllib(url_curr)\n","\n","\n","  h4_tag = bs.find_all(SITES['yelp']['search_results_tags'], class_= SITES['yelp']['search_results_tags_class'])\n","  #h4_tag = bs.find_all('h4', {'class':'css-1l5lt1i'}) # this equivalent to the line above.\n","  #print(h4_tag)\n","  #for h in h4_tag:\n","  #  print(h.prettify())\n","  for h in h4_tag:\n","    if h.find('a')['href'][0:9] != '/adredir?':\n","      lst.append(SITES['yelp']['url'] + h.find('a')['href'])\n","\n","  # get the next page anchor on current page, then go to the next page recursively until there are no more pages.\n","  next_page = bs.find('a', {'class' : 'next-link navigation-button__09f24__1EzzD css-166la90'})\n","  if next_page != None:\n","    get_yelp_search_result_links(next_page['href'], lst)\n","\n","  # end method"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUEMuh-Ry2a3"},"source":["the next set of cells collects establishment links and saves them to a file to load later."]},{"cell_type":"code","metadata":{"id":"e8a5qM74Xcko","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641773507733,"user_tz":360,"elapsed":7147,"user":{"displayName":"Austin Faulkner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiL4U4eSCdMz9tJIdaqafSrskkrgehAQ-ugtFxu-w=s64","userId":"13684566176525076719"}},"outputId":"7a91e282-f23d-4ac6-aa8c-7946ba95ba9e"},"source":["distillery_url_list = []    \n","print('Gathering \"Distilleries\" search results ')\n","get_yelp_search_result_links(SITES['yelp']['url'] + SITES['yelp']['austin_distillery_search_ext'], distillery_url_list)\n","for url in distillery_url_list:\n","  print('Found link to ' + url)\n","print('Number of Distillery urls: ' + str(len(distillery_url_list)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gathering \"Distilleries\" search results \n","Gathering search result urls @ https://www.yelp.com/search?find_desc=distilleries&find_loc=Austin%2C+TX\n","Compiling BS object at url @ https://www.yelp.com/search?find_desc=distilleries&find_loc=Austin%2C+TX\n","Number of Distillery urls: 0\n"]}]},{"cell_type":"code","metadata":{"id":"azTeykH4G79h","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"error","timestamp":1641773507976,"user_tz":360,"elapsed":245,"user":{"displayName":"Austin Faulkner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiL4U4eSCdMz9tJIdaqafSrskkrgehAQ-ugtFxu-w=s64","userId":"13684566176525076719"}},"outputId":"848bce2e-800d-474b-8b52-a6cf03035fb6"},"source":["with open(ROOT + '/Data_Collecting/distillery_links.txt', 'w', encoding='utf-8') as f:\n","  for url in distillery_url_list:\n","    f.write(url + '\\n')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-997157b83583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Data_Collecting/distillery_links.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistillery_url_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gdrive/MyDrive/Austin_Drinks/Data_Collecting/distillery_links.txt'"]}]},{"cell_type":"code","metadata":{"id":"RMtxJTklXq9Y"},"source":["winery_url_list = []    \n","print('Gathering \"Wineries\" search results ')\n","get_yelp_search_result_links(SITES['yelp']['url'] + SITES['yelp']['austin_winery_search_ext'], winery_url_list)\n","for url in winery_url_list:\n","  print('Found link to ' + url)\n","print('Number of Winery urls: ' + str(len(winery_url_list)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rnI1wgDGG_nI"},"source":["with open(ROOT + '/Data_Collecting/winery_links.txt', 'w', encoding='utf-8') as f:\n","  for url in winery_url_list:\n","    f.write(url + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jtdwgk0kXyE5"},"source":["brewery_url_list = []    \n","print('Gathering \"Breweries\" search results ')\n","get_yelp_search_result_links(SITES['yelp']['url'] + SITES['yelp']['austin_brewery_search_ext'], brewery_url_list)\n","for url in brewery_url_list:\n","  print('Found link to ' + url)\n","print('Number of Brewery urls: ' + str(len(brewery_url_list)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMXLvA5CHAqq"},"source":["with open(ROOT + '/Data_Collecting/brewery_links.txt', 'w', encoding='utf-8') as f:\n","  for url in brewery_url_list:\n","    f.write(url + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRkRzU7hJymc"},"source":["Cells to load links from file"]},{"cell_type":"code","metadata":{"id":"Bxz1swlcJ1h8"},"source":["distillery_url_list = []\n","winery_url_list = []   \n","brewery_url_list = []   \n","\n","with open(ROOT + '/Data_Collecting/distillery_links.txt', 'r', encoding='utf-8') as f:\n","  for line in f.readlines():\n","    distillery_url_list.append(line)\n","\n","with open(ROOT + '/Data_Collecting/winery_links.txt', 'r', encoding='utf-8') as f:\n","  for line in f.readlines():\n","    winery_url_list.append(line)\n","\n","with open(ROOT + '/Data_Collecting/brewery_links.txt', 'r', encoding='utf-8') as f:\n","  for line in f.readlines():\n","    brewery_url_list.append(line)\n","\n","print('distilleries:  ' + str(len(distillery_url_list)))\n","print('wineries:      ' + str(len(winery_url_list)))\n","print('breweries:     ' + str(len(brewery_url_list)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7e8-kEtllz-"},"source":["Function that retrieves an establishment's information from yelp via JSON and the page itself.\n","\n","TODO Separate out to only collect JSON data\n","Make a separate function to save into csv files\n"]},{"cell_type":"code","source":["# instead of collecting a list of htmls for each link,\n","# it might be better to save space and collect information\n","# we need from each site one at a time,\n","# then offload it into a file or DB somewhere..\n","\n","def get_yelp_establishment_information(link, infoCSV, infoJSON, SITES):\n","\n","  print('Attempting to collect from ' + link)\n","\n","  html = getBS_via_selenium(link, \n","                            onclickButton = True, \n","                            button_text = SITES['yelp']['amenities_reveal_button_text'], \n","                            hider_attr = SITES['yelp']['amenities_reveal_button_attr'], \n","                            button_attr = SITES['yelp']['amenities_reveal_button_attr2'], \n","                            button_attr_def = SITES['yelp']['amenities_reveal_button_attr2_def'])\n","  \n","  # what information do we want? json script tags may be useful here, too\n","\n","  # script tag we want: div class = 'main-content-wrap main-content-wrap--full' only one of these tags\n","\n","    #                                  main-content-wrap main-content-wrap--full\n","  div_tag = html.find('div', class_ = 'main-content-wrap main-content-wrap--full')\n","  script_tags = div_tag.find_all('script', {'type' : 'application/ld+json'})\n","\n","  #print(len(script_tags))\n","  #for st in script_tags:\n","  #  print(st.prettify())\n","  # looks like it is the second script_tag with json information we want.  \n","\n","\n","  # if a json is available on the web page, we get that json and store it here\n","  # we collect whatever information may be available in that json.\n","  infoJSON = None\n","  try:\n","    # strict omits control characters when True.. or change to false and re.sub with ' ' instead.\n","    infoJSON = json.loads(''.join(script_tags[1].contents), strict = True)\n","  except:\n","    print('Unable to get data from json, saving failed link and moving forward..')\n","    __errorLog(link, 'JSON data not found')\n","  else:\n","    # extract info in JSON\n","    # name of establishment\n","\n","    name = ''\n","    if 'name' in infoJSON:\n","      infoJSON['name'] = cleanName(infoJSON['name'])\n","      name = infoJSON['name']\n","      infoCSV.append(infoJSON['name'])\n","      print('Collecting information for ' + name)\n","    else:\n","      print('No name for url ' + link)\n","      infoCSV.append('unnamed establishment')\n","      __errorLog(link, 'JSON data has no \\'name\\' key')\n","\n","    # address\n","    if 'address' in infoJSON and 'streetAddress' in infoJSON['address']:\n","      streetAddress = re.sub('\\n', '', infoJSON['address']['streetAddress'])\n","      infoCSV.append(streetAddress)\n","    else:\n","      print('No street address available for ' + name)\n","      infoCSV.append('')\n","\n","    if 'address' in infoJSON and 'addressLocality' in infoJSON['address']:\n","      localAddress = infoJSON['address']['addressLocality']\n","      infoCSV.append(localAddress)\n","    else:\n","      print('No local address available for ' + name)\n","      infoCSV.append('')\n","\n","    if 'address' in infoJSON and 'addressRegion' in infoJSON['address']:\n","      regionAddress = infoJSON['address']['addressRegion']\n","  #   print(regionAddress)\n","      infoCSV.append(regionAddress)\n","    else:\n","      print('No region address available for ' + name)\n","      infoCSV.append('')\n","\n","    if 'address' in infoJSON and 'postalCode' in infoJSON['address']:\n","      postalCode = infoJSON['address']['postalCode']\n","  #   print(postalCode)\n","      infoCSV.append(postalCode)\n","    else:\n","      print('No postal code available for ' + name)\n","      infoCSV.append('')\n","    \n","    # phone number\n","    if 'telephone' in infoJSON:\n","      telephone = infoJSON['telephone']\n","  #   print(telephone)\n","      infoCSV.append(telephone)\n","    else:\n","      print('No telephone available for ' + name)\n","      infoCSV.append('')\n","\n","    # dollar signs/price range\n","    if 'priceRange' in infoJSON:\n","      priceRange = infoJSON['priceRange']\n","  #   print('price range: ' + priceRange)\n","      infoCSV.append(priceRange)\n","    else:\n","      print('No price range available for ' + name)\n","      infoCSV.append('')\n","\n","    # review rating\n","    if 'aggregateRating' in infoJSON and 'ratingValue' in infoJSON['aggregateRating']:\n","      ratingValue = infoJSON['aggregateRating']['ratingValue']\n","  #   print('rating value: ' + str(ratingValue))\n","      infoCSV.append(ratingValue)\n","    else:\n","      print('No rating value available for ' + name)\n","      infoCSV.append('')\n","\n","    # number of reviews\n","    if 'aggregateRating' in infoJSON and 'reviewCount' in infoJSON['aggregateRating']:\n","      ratingCount = infoJSON['aggregateRating']['reviewCount']\n","  #   print('rating count: ' + str(ratingCount))\n","      infoCSV.append(ratingCount)\n","    else:\n","      print('No rating count available for ' + name)\n","      infoCSV.append('')\n","\n","    # website\n","\n","    # <div class = ' css-1vhakgw border--top__373c0__19Owr border-color--default__373c0__2oFDT'>\n","      # <a>\n","    div_tag = html.find('div', class_ = ' css-1vhakgw border--top__373c0__2SDyx border-color--default__373c0__1WSID')\n","    if div_tag.find('a'):\n","      website = div_tag.find('a').get_text()\n","      infoCSV.append(website)\n","      infoJSON['website'] = website\n","    else:\n","      print('No website available for ' + name)\n","      infoCSV.append('')\n","      \n","    # est. type.\n","    span_tags = html.find_all('span', {'class' : ' display--inline__373c0__3d-lf margin-r1__373c0__7ZINV border-color--default__373c0__2s5dW'})\n","    tags = []\n","    for span in span_tags:\n","      anc_tags = span.find_all('a')\n","  #    print(str(len(anc_tags)))   # there could be zero or more anchor tags to get text from..\n","      if anc_tags:\n","        est_str = ''\n","        for anc in anc_tags:\n","          if anc.get_text() != 'Unclaimed':\n","            est_str += anc.get_text() + '|'    # cannot use commas as a delimiter because we are currently saving to csv format.\n","            tags.append(anc.get_text())\n","        if est_str:\n","          infoCSV.append(est_str)\n","  #        print(est_str)\n","        else:\n","          print('No type string available for ' + name)\n","\n","    infoJSON['tags'] = tags\n","\n","    # hours of operations, if available  \n","      # this is specifically for yelp.com.. so move to the dictionary for yelp.com once it is working.\n","      # <table class = ' hours-table__373c0__1S9Q_ table__373c0__1paZL table--simple__373c0__3hEOO' ...>\n","      # yes there is a space at the front\n","        # <tr class = ' table-row__373c0__3xT3x' ...>\n","          # <th class = ' table-header-cell__373c0__OywTx' ...> gets the day of the week\n","          # <td class = ' table-cell__373c0__Hc_7A table-cell--top__373c0__1hgmO' ...> gets the hours of the day\n","          # and td of same class above also gets for current day \"Open\", \"Closed now\", etc.. if text is present, blank otherwise.\n","\n","    data = {}\n","    table = html.find('table', {'class' : ' hours-table__373c0__2YHlD table__373c0__1FIZ8 table--simple__373c0__3QsR_'})\n","    if table.find('tbody').contents:\n","      table_row = table.find_all('tr', {'class' : ' table-row__373c0__1F6B0'})\n","  #    arr = []\n","      day = ''\n","      hours = ''\n","      for row in table_row:\n","        day_str = ''\n","        hours_str = ''\n","        # get th and td\n","        if row.find('th'):\n","          day_str = row.find('th').get_text()\n","          day += day_str + '|'\n","        else:\n","          day += '|'\n","        if row.find('td'):\n","          hours_str = row.find('td').get_text()\n","          hours += hours_str + '|'\n","        else:\n","          hours += '|'\n","        \n","        data[day_str] = hours_str\n","      infoJSON['Hours'] = data\n","      infoCSV.append(day)\n","      infoCSV.append(hours)\n","  #      a = []\n","  #      a.append(day)   # 1 column\n","  #      a.append(hours)   # 1 column, only getting the first of two.\n","  #      arr.append(a)  \n","  #    data = pd.DataFrame(arr, columns = ['Day', 'Hours'])\n","  #    print(data)\n","    else:\n","      print('No hours table is available for ' + name) \n","      # empty columns for 'day' and 'hours'.\n","      for i in range(0, 2): \n","        infoCSV.append('||||||')\n","\n","\n","    # Amenities, if it exists.\n","    if 'Amenities and More' in html.text:\n","      amenities_tags = html.find_all('span', {'class': ' css-1h1j0y3'})     # what the company accommodates\n","      anti_amenities_tags = html.find_all('span', {'class': ' css-chtywg'}) # what the company does not accommodate\n","      infoJSON['Accommodations'] = [tag.text for tag in amenities_tags]\n","      infoJSON['Unaccommodations'] = [tag.text for tag in anti_amenities_tags]\n","    else:\n","      print('No button containing \\\"Amenities\\\" exists on this page.')\n","\n","    # menu, if available?\n","\n","    # about the business?\n","\n","  # end method"],"metadata":{"id":"-DF3vfZtEZ9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eaa3JNL-eBOw"},"source":["Save data to CSV file.  Also saves JSON files"]},{"cell_type":"code","metadata":{"id":"lBN36FMiY5Vi"},"source":["# initial saving method.  \n","# Now that files are created, develop methods to modify existing files as necessary.\n","\n","def save_data_to_csv(urls, fileName, mode):\n","  alreadySaved = os.listdir(ROOT + '/Data_Collecting/JSON files') # used to check that we do not already have a JSON saved\n","  with open(ROOT + '/Data_Collecting/' + fileName + '.csv', mode) as f:\n","    writer = csv.writer(f)\n","    header = ['Name', 'Address', 'City', 'State', 'Zip', 'Phone', 'Price_Range', 'Ratings', 'Review_Count', 'URL', 'Tag', 'Day', 'Hours']\n","    writer.writerow(header)\n","    counter = 0;\n","    for url in urls:\n","      counter += 1\n","      print('count: ' + str(counter))\n","      est_info = []\n","      get_yelp_establishment_information(url, est_info, alreadySaved)\n","      print()\n","      writer.writerow(est_info)\n","  f.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEJRGvroX5RZ"},"source":["save_data_to_csv(distillery_url_list, 'Distillery_Data', 'w') # or 'a' for appending to file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOzuJr6fYAnh"},"source":["save_data_to_csv(winery_url_list, 'Winery_Data', 'w') # or 'a' for appending to file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27aaTqpGYIbh"},"source":["save_data_to_csv(brewery_url_list, 'brewery_Data', 'w') # or 'a' for appending to file"],"execution_count":null,"outputs":[]}]}