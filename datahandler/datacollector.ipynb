{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"datacollector.ipynb","provenance":[],"collapsed_sections":["9SQ8_oo_4tjc","10c_EBEYLtQa","osyGTWFv27cd","cClPYQEl4nrd","oAXDqvVA-0K5"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Collector Package\n","Designed to collect data needed for Austin Drinks\n","\n","---\n","\n","Imports and Constants:\n","* Imports the necessary packages and modules\n","\n","Utility Methods:\n","* Methods for saving and loading data, logging errors, cleaning strings, and other miscellaneous methods\n","\n","Gather Establishment Information Methods:\n","* Methods designed to work with yelp-specific website layout\n","\n","Web Driver Method:\n","* Method used to create BeautifulSoup object and returns object to the caller\n","\n","Data Collecting Methods:\n","* Higher-level methods that utilize other package methods to complete a task or sequence of tasks\n","\n","\n","---\n"],"metadata":{"id":"ERBIEClt4PXO"}},{"cell_type":"markdown","source":["# TODO:\n","---\n","General:\n","* Clean up code to make consistent and add/remove comments where necessary\n","---\n","Utility Methods:\n","---\n","Gather Establishment Information Methods:\n","* Extract Method refactor code so that individual components of information is gathered in their own methods\n","---\n","Web Driver Method:\n","* add code using proxies to speed up the data collection process and avoid being flagged\n","---\n","Data Collecting Method:\n","---\n","Google:\n","* start probing Google to see how to scrape data\n","---"],"metadata":{"id":"dLzsUxT3HFI-"}},{"cell_type":"markdown","source":["# Imports and Constants"],"metadata":{"id":"9SQ8_oo_4tjc"}},{"cell_type":"code","source":["from selenium.webdriver.support.ui import WebDriverWait       \n","from selenium.webdriver.common.by import By       \n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from bs4 import BeautifulSoup\n","from datetime import datetime as dt\n","import csv\n","import io\n","import json\n","import os\n","import random\n","import re\n","import time\n","SITES = None\n","URL_LIST_FILENAMES = []\n","CITY_JSON_FILENAMES = []\n","ROOT = None"],"metadata":{"id":"b6oq4ov0i_Qp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utility Methods\n","\n","Methods for saving and loading data, logging errors, and other miscellaneous tasks\n","\n","---\n","* __add_filename_to_url_list\n","* __add_filename_to_city_json_list\n","* get_url_list_filenames\n","* get_city_json_filenames\n","* set_constants\n","* __errorLog (typically used by other methods to log errors during the web scraping process)\n","* save_city_csv\n","* reformat_json\n","* save_city_json\n","* load_city_json\n","* save_url_list\n","* load_url_list\n","* clean_string\n","---"],"metadata":{"id":"10c_EBEYLtQa"}},{"cell_type":"code","source":["def __add_filename_to_url_list(filename):\n","  global URL_LIST_FILENAMES\n","  if filename not in URL_LIST_FILENAMES:\n","    URL_LIST_FILENAMES.append(filename)"],"metadata":{"id":"KVOi2QTrLMSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def __add_filename_to_city_json_list(filename):\n","  global CITY_JSON_FILENAMES\n","  if filename not in CITY_JSON_FILENAMES:\n","    CITY_JSON_FILENAMES.append(filename)"],"metadata":{"id":"3t89xAwzwWin"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_url_list_filenames():\n","  global URL_LIST_FILENAMES\n","  return URL_LIST_FILENAMES"],"metadata":{"id":"XzhY5xHah_By"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_city_json_filenames():\n","  global CITY_JSON_FILENAMES\n","  return CITY_JSON_FILENAMES"],"metadata":{"id":"5lpjxq8E0f2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_constants(dic, root):\n","  global ROOT\n","  ROOT = root\n","  print('root dir: ' + ROOT)\n","  global SITES\n","  SITES = dic\n","  print(SITES)\n","  global URL_LIST_FILENAMES\n","  URL_LIST_FILENAMES = os.listdir(root + '/Data/search_result_links')\n","  print(URL_LIST_FILENAMES)\n","  global CITY_JSON_FILENAMES\n","  CITY_JSON_FILENAMES = os.listdir(root + '/Data/json_by_city')\n","  print(CITY_JSON_FILENAMES)"],"metadata":{"id":"EOd4BTcOexYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def __errorLog(link, message):\n","  '''\n","  # log errors as they occur in collection process\n","  # all errors are appended at the end with a date, time, and error message\n","  '''\n","  global ROOT\n","  with open(ROOT + '/Data/error_log.txt', 'a', encoding='utf-8') as f:\n","    f.write('{}\\t\\t{}\\n\\t{}\\n\\n'.format(dt.now(), link, message))"],"metadata":{"id":"76ZmgNAgMXvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_city_csv(filename, data, mode = 'w'):\n","  '''\n","  # save collected data for a city to csv\n","  '''\n","  global ROOT\n","  if not filename.endswith('.csv'):\n","    filename += '.csv'\n","  with open(ROOT + '/Data/csv_by_city/' + filename, mode, encoding='utf=8') as f:\n","    header = ['name', 'address', 'address', 'city', 'state', 'postal_code', \n","                  'telephone', 'price', 'rating_value', 'review_count', 'status', \n","                  'longitude', 'latitude', 'website', 'tags', 'days', \n","                  'hours', 'accommodations', 'unaccommodations']\n","    writer = csv.writer(f)\n","    writer.writerow(header)\n","    for row in data:\n","      writer.writerow(row)"],"metadata":{"id":"cEd863eBiplb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reformat_json(data):\n","  '''\n","  # reformat json data to a preferred readable format\n","  '''\n","  new_data = {}\n","  new_data['name'] = data['name']\n","  new_data['open_for_business'] = data['open_for_business']\n","  new_data['tags'] = data['tags']\n","  new_data['website'] = data['website']\n","  new_data['longitude'] = data['longitude']\n","  new_data['latitude'] = data['latitude']\n","  new_data['address'] = data['address']\n","  new_data['postal_code'] = data['postal_code']\n","  new_data['city'] = data['city']\n","  new_data['state'] = data['state']\n","  new_data['telephone'] = data['telephone']\n","  new_data['price'] = data['price']\n","  new_data['hours'] = data['hours']\n","  new_data['accommodations'] = data['accommodations']\n","  new_data['unaccommodations'] = data['unaccommodations']\n","  new_data['rating_value'] = data['rating_value']\n","  new_data['review_count'] = data['review_count']\n","  new_data['reviews'] = data['reviews']\n","  return new_data"],"metadata":{"id":"9VCIt79cpaIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_city_json(filename, data):\n","  '''\n","  # save organized JSON data collected from site\n","  '''\n","  global ROOT\n","  if not filename.endswith('.json'):\n","    filename += '.json'\n","  with io.open(ROOT + '/Data/json_by_city/' + filename, 'w', encoding='utf-8') as f:\n","    json.dump(data, f, ensure_ascii = False, indent = 4)\n","  print('city json saved to: ' + ROOT + '/Data/json_by_city/' + filename)\n","  __add_filename_to_city_json_list(filename)"],"metadata":{"id":"krC3kvOLLyUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_city_json(filename):  \n","  '''\n","  # load and return JSON data from file\n","  '''\n","  global ROOT\n","  with io.open(ROOT + '/Data/json_by_city/' + filename, 'r', encoding='utf-8') as f:\n","    data = json.load(f)\n","  return data"],"metadata":{"id":"cDcZpPWdMKKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_url_list(filename, data, mode = 'w'):\n","  '''\n","  # Saves a list of urls to a text file to reduce web scraping time\n","  # allows the users to use their own naming scheme on the file\n","  '''\n","  global ROOT\n","  if not filename.endswith('.txt'):\n","    filename += '.txt'\n","  with open(ROOT + '/Data/search_result_links/' + filename, mode, encoding='utf-8') as f:\n","    for url in data:\n","      f.write(url + '\\n')\n","  print('URLs saved to: ' + ROOT + '/Data/search_result_links/' + filename)\n","  __add_filename_to_url_list(filename)"],"metadata":{"id":"M9xrSd_8kwVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_url_list(filename):\n","  '''\n","  # read from a text file the list of urls saved and store them in list(data)\n","  '''\n","  global ROOT\n","  if not filename.endswith('.txt'):\n","    filename += '.txt'\n","  data = []\n","  with open(ROOT + '/Data/search_result_links/' + filename, 'r', encoding='utf-8') as f:\n","    for line in f.readlines():\n","      line = re.sub('\\n', '', line)\n","      data.append(line)\n","  return data"],"metadata":{"id":"Mv4aO5YRRzx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_string(string):\n","  '''\n","  # clean up the string of unwanted characters\n","  '''\n","  cleaned = re.sub('&amp;', 'and', string)\n","  cleaned = re.sub('&apos;', '\\'', cleaned)\n","  cleaned = re.sub('\\n', '', cleaned)\n","  return cleaned"],"metadata":{"id":"LRR_R97CNZ1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_review_string(string):\n","  '''\n","  # clean up the string of unwanted characters\n","  '''\n","  cleaned = re.sub('&amp;', 'and', string)\n","  cleaned = re.sub('&apos;', '\\'', cleaned)\n","  cleaned = re.sub('&quot;', '\"', cleaned)\n","  cleaned = re.sub('\\n', ' ', cleaned)\n","  return cleaned"],"metadata":{"id":"aey39s32b-a2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Gather Establishment Information Methods\n","\n","Methods that focus on accommodating individual website's layouts to obtain establishment information\n","\n","---\n","* __get_value_in_SITES\n","* __get_value_from_json\n","* __get_yelp_est_info\n","---"],"metadata":{"id":"osyGTWFv27cd"}},{"cell_type":"code","source":["def __get_value_in_SITES(site, key):\n","  '''\n","  # Access SITES dictionary given site and key values\n","  # returns None if not in dictionary\n","  '''\n","  global SITES\n","  if site in SITES and key in SITES[site]:\n","    return SITES[site][key]\n","  else:\n","    return None"],"metadata":{"id":"7ufT2GuezfxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def __get_value_from_json(data, first_layer_key, second_layer_key = None):\n","  # check if first_key is in data\n","  if first_layer_key in data:\n","    # check if second_layer_key and if second_key is in data[first_layer_key]\n","    if second_layer_key is not None and second_layer_key in data[first_layer_key]:\n","      if isinstance(data[first_layer_key][second_layer_key], str):\n","        return clean_string(data[first_layer_key][second_layer_key])\n","      else:\n","        return data[first_layer_key][second_layer_key]\n","    # if no second_layer_key, return value for first_layer_key in data\n","    else:\n","      if isinstance(data[first_layer_key], str):\n","        return clean_string(data[first_layer_key])\n","      else:\n","        return data[first_layer_key]\n","  return ''"],"metadata":{"id":"oeRWiHvtwjeZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def __get_yelp_est_info(site, link):\n","  '''\n","  # Method specifically tailored to yelp for collecting data from an establishment's page\n","  '''\n","  global SITES, PERMANENTLY_CLOSED, OPEN_FOR_BUSINESS\n","  siteJSON = None\n","  infoJSON = {}\n","  infoCSV = []  \n","  html = getBS(link, site)\n","  if html is not None:\n","\n","    # get script tags of interest\n","    # pretty standard for script tags of interest\n","    tags = html.find_all('script', {'type' : 'application/ld+json'}) \n","\n","    # which tag in the list of tags do we need?\n","    # whichever contains 'name' in the first set of keys for the json.\n","    \n","    for tag in tags:\n","      if 'name' in json.loads(''.join(tag.contents)):\n","        siteJSON = json.loads(''.join(tag.contents))\n","        break\n","        \n","    # extract information in the JSON\n","    if siteJSON is not None:\n","\n","      # name of establishment\n","      infoJSON['name'] = __get_value_from_json(siteJSON, 'name')\n","      if not infoJSON['name']:\n","        infoJSON['name'] = clean_string(html.find(SITES[site]['establishment_name_tag'], SITES[site]['establishment_name_tag_attr_and_def']).text)\n","      infoCSV.append(infoJSON['name'])\n","      print('Collecting information for ' + infoJSON['name'])\n","\n","      # address\n","      infoJSON['address'] = __get_value_from_json(siteJSON, 'address', 'streetAddress')\n","      if 'Get Directions' in infoJSON['address']:\n","        infoJSON['address'] = ''\n","      infoCSV.append(infoJSON['address'])\n","\n","      # city\n","      infoJSON['city'] = __get_value_from_json(siteJSON, 'address', 'addressLocality')\n","      infoCSV.append(infoJSON['city'])\n","\n","      # state\n","      infoJSON['state'] = __get_value_from_json(siteJSON, 'address', 'addressRegion')\n","      infoCSV.append(infoJSON['state'])\n","\n","      # postal code\n","      infoJSON['postal_code'] = __get_value_from_json(siteJSON, 'address', 'postalCode')\n","      infoCSV.append(infoJSON['postal_code'])\n","      \n","      # phone number\n","      infoJSON['telephone'] = __get_value_from_json(siteJSON, 'telephone')\n","      infoCSV.append(infoJSON['telephone'])\n","\n","      # dollar signs/price range\n","      infoJSON['price'] = __get_value_from_json(siteJSON, 'priceRange')\n","      infoCSV.append(infoJSON['price'])\n","\n","      # review rating\n","      infoJSON['rating_value'] = __get_value_from_json(siteJSON, 'aggregateRating', 'ratingValue')\n","      infoCSV.append(infoJSON['rating_value'])\n","\n","      # number of reviews\n","      infoJSON['review_count'] = __get_value_from_json(siteJSON, 'aggregateRating', 'reviewCount')\n","      infoCSV.append(infoJSON['review_count'])\n","\n","      # reviews on establishment\n","      infoJSON['reviews'] = __get_value_from_json(siteJSON, 'review')\n","\n","      # for some reason in Yelp's json data \n","      # each reviewer's rating value is nested into another dictionary by itself\n","      # let's pull it out and have one less layer deep to go for it..\n","      for review in infoJSON['reviews']:\n","        value = review['reviewRating']['ratingValue']\n","        review['reviewRating'] = value\n","        review['description'] = clean_review_string(review['description'])\n","\n","      # menu or cuisine, if available?\n","      infoJSON['cuisine'] = __get_value_from_json(siteJSON, 'servesCuisine')\n","\n","      # end json data extraction\n","\n","#    else:\n","      # extract information from the page itself\n","### TODO: EXTRACT INFORMATION FROM THE PAGE ITSELF IF JSON CANNOT BE FOUND\n","    # extracted data not in available json\n","\n","    # check if the establishment is permanently closed\n","    if SITES[site]['status_text'] in html.text.lower():\n","      infoJSON['open_for_business'] = 0\n","    else:\n","      infoJSON['open_for_business'] = 1\n","    infoCSV.append(infoJSON['open_for_business'])\n","\n","    # map location\n","    tag = html.find(SITES[site]['location_tag'], SITES[site]['location_tag_attr_and_def'])\n","    longitude = 0.0\n","    latitude = 0.0\n","    if tag is not None and tag.find('img')['src']:\n","      string = tag.find('img')['src']\n","      # %2C is the divider between longitude and latitude\n","      # there are multiple coordinates in this string\n","      # and the correct coordinates follow .png towards the end of the string.\n","      # layout: .png%7C00.000000%2C-00.000000&\n","      # .png%7C[]%2C[]&\n","      regex = re.search(SITES['yelp']['location_string_start'] + '+[A-Z0-9%.-]+' + SITES['yelp']['location_string_end'], string)\n","\n","      if regex is not None:\n","        regex_str = regex.group(0)\n","        regex_str = re.sub(SITES['yelp']['location_string_start'], '', regex_str)\n","        regex_str = re.sub(SITES['yelp']['location_string_end'], '', regex_str)\n","        long_lat = regex_str.split(SITES['yelp']['location_string_delimiter'])\n","        longitude = long_lat[0]\n","        latitude = long_lat[1]\n","    infoJSON['longitude'] = float(longitude)\n","    infoJSON['latitude'] = float(latitude)\n","    infoCSV.append(longitude)\n","    infoCSV.append(latitude)\n","\n","    # website\n","    # <div class = ' css-1vhakgw border--top__373c0__19Owr border-color--default__373c0__2oFDT'>\n","      # <a>\n","    tag = html.find(SITES[site]['business_url_tag'], SITES[site]['business_url_tag_attr_and_def'])\n","    website = ''\n","    if tag is not None and tag.find('a'):\n","      website = tag.find('a').get_text()\n","    infoJSON['website'] = website\n","    infoCSV.append(website)\n","      \n","    # est. type.\n","    tags = html.find_all(SITES[site]['est_tags_tag'], SITES[site]['est_tags_tag_attr_and_def'])\n","    biz_tags = []\n","    tag_str = ''\n","    for tag in tags:\n","      anc_tags = tag.find_all('a')\n","      if anc_tags:\n","        for anc in anc_tags:\n","          if anc.get_text() != 'Unclaimed':\n","            tag_str += anc.get_text() + '|'    # cannot use commas as a delimiter because we are currently saving to csv format.\n","            biz_tags.append(anc.get_text())\n","\n","    infoJSON['tags'] = biz_tags      \n","    infoCSV.append(tag_str)\n","\n","    # hours of operations, if available  \n","      # this is specifically for yelp\n","      # <table class = ' hours-table__373c0__1S9Q_ table__373c0__1paZL table--simple__373c0__3hEOO' ...>\n","      # yes, there is a space at the front\n","        # <tr class = ' table-row__373c0__3xT3x' ...>\n","          # <th class = ' table-header-cell__373c0__OywTx' ...> gets the day of the week\n","          # <td class = ' table-cell__373c0__Hc_7A table-cell--top__373c0__1hgmO' ...> gets the hours of the day\n","          # and td of same class above also gets for current day \"Open\", \"Closed now\", etc.. if text is present, blank otherwise.\n","\n","    days_hours = {}\n","    day = ''\n","    hours = ''\n","    table = html.find(SITES[site]['hoO_tag'], SITES[site]['hoO_tag_attr_and_def'])\n","    if table is not None and table.find('tbody').contents:\n","      table_row = table.find_all('tr', SITES[site]['hoO_row_tag_attr_and_def'])\n","  #    arr = []\n","      for row in table_row:\n","        day_str = ''\n","        hours_str = ''\n","        # get th and td\n","        if row.find('th'):\n","          day_str = row.find('th').get_text()\n","          day += day_str + '|'\n","        else:\n","          day += '|'\n","        if row.find('td'):\n","          hours_str = row.find('td').get_text()\n","          hours_str = re.sub('(Next day)', '', hours_str)\n","          hours += hours_str + '|'\n","        else:\n","          hours += '|'        \n","        days_hours[day_str] = hours_str\n","  #      a = []\n","  #      a.append(day)   # 1 column\n","  #      a.append(hours)   # 1 column, only getting the first of two.\n","  #      arr.append(a)  \n","  #    data = pd.DataFrame(arr, columns = ['Day', 'Hours'])\n","  #    print(data)\n","    else:\n","#        print('No hours table is available for ' + name) \n","      # empty columns for 'day' and 'hours'.\n","      day = '||||||'\n","      hours = '||||||'\n","\n","    # even if days_hours is empty, append it to the json file.\n","    infoJSON['hours'] = days_hours\n","    infoCSV.append(day)\n","    infoCSV.append(hours)\n","\n","    # Amenities, if it exists.\n","    accommodation_list = []\n","    unaccommodation_list = []\n","    accomm_str = ''\n","    unaccom_str = ''\n","    if SITES[site]['amenities_header_text'] in html.text:\n","      # what the company accommodates\n","      amenities_tags = html.find_all(SITES[site]['amenities_tag'], SITES[site]['amenities_tag_attr_and_def'])\n","      # what the company does not accommodate\n","      anti_amenities_tags = html.find_all(SITES[site]['anti_amenities_tag'], SITES[site]['anti_amenities_tag_attr_and_def'])\n","      accommodation_list = [tag.text for tag in amenities_tags]\n","      unaccommodation_list = [tag.text for tag in anti_amenities_tags]\n","      for tag in amenities_tags:\n","        accomm_str += tag.text + '|'\n","      for tag in anti_amenities_tags:\n","        unaccom_str += tag.text + '|'\n","    infoJSON['accommodations'] = accommodation_list\n","    infoJSON['unaccommodations'] = unaccommodation_list\n","    infoCSV.append(accomm_str)\n","    infoCSV.append(unaccom_str)\n","      # about the business?\n","\n","\n","  else:\n","    print('Error getting BeautifulSoup object for ' + url_curr)\n","    __errorLog(url_curr, 'Error getting BeautifulSoup object in __get_yelp_est_info')\n","\n","  # reformat the json structure for readability before returning the object\n","  return infoCSV, reformat_json(infoJSON)\n","  # end method"],"metadata":{"id":"OCPqLyzJ276F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Web Driver Creates BeautifulSoup Object Method\n","Method to create beautifulsoup object and returns object to the caller\n","\n","---\n","* __getBS_yelp_button_press\n","* getBS\n","* get_search_result_links\n","---"],"metadata":{"id":"cClPYQEl4nrd"}},{"cell_type":"code","source":["def __getBS_yelp_button_press(driver, url, source, site):\n","  '''\n","  # Method utilized to press the Attributes button to reveal all Attributes for an establishment\n","  '''\n","  global SITES\n","  \n","  # if we are interested in clicking a button,\n","  # we repeat the process using the existing driver to click the button\n","  attempts = 0\n","  max_attempts = 2\n","  unsuccessful = True \n","  while unsuccessful:\n","    attempts += 1\n","    try:\n","      # this aria-controls is unique to the button we are looking for..\n","#             buttons = source.find_all('button', {'class': ' css-174jypu'})  # find buttons with class of interest\n","      buttons = source.find_all('button', {SITES[site]['amenities_reveal_button_tag_attr'] : SITES[site]['amenities_reveal_button_tag_attr_def']})\n","      for button in buttons:\n","        if SITES[site]['amenities_reveal_button_text'] in button.text:\n","          driver.execute_script(\n","              \"arguments[0].click();\", \n","              WebDriverWait(driver, 20).until(EC.element_to_be_clickable(\n","                  (By.CSS_SELECTOR, \n","                   'button[' + SITES[site]['amenities_reveal_button_hider_attr'] + '=' + button[SITES[site]['amenities_reveal_button_hider_attr']] + ']'))))\n","          time.sleep(random.randint(3,6))\n","          break\n","    except BaseException as ex:\n","      if attempts < max_attempts:\n","        print('Unable to click button. Retrying in 30 seconds. \\\n","              See exception repr for details: ')\n","        print(repr(ex))      # print object as string\n","        time.sleep(30)\n","      else:\n","        print('Too many attempts to click button, breaking loop')\n","        #log error to file.\n","        __errorLog(url, 'Exceeded click button request limit to extract Amenities in getBS_yelp_button_press\\n' + repr(ex))\n","        break\n","    else:\n","      unsuccessful = False\n","      new_source = BeautifulSoup(driver.page_source, 'html.parser')   \n","       \n","  return new_source\n"],"metadata":{"id":"MRrokpA6AASp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getBS(url, site = None):\n","  '''\n","  # Selenium web driver is used to get a webpage's html and parse it to a BeautifulSoup object\n","  '''\n","  global SITES\n","\n","  chrome_options = Options()\n","  chrome_options.add_argument('--headless') # don't spawn window\n","  chrome_options.add_argument('--no-sandbox')\n","  chrome_options.add_argument('--disable-dev-shm-usage')\n","  chrome_options.add_argument('user-agent = Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36')\n","  driver = webdriver.Chrome(executable_path = '/usr/lib/chromium-browser/chromedriver', options = chrome_options)\n","  print('Building BS object at url @ ' + url)\n","\n","  # first, get the page where the button we want is at\n","  source = None\n","  attempts = 0\n","  max_attempts = 4\n","  unsuccessful = True \n","  while unsuccessful:\n","    attempts += 1\n","    try:\n","      driver.get(url)\n","      time.sleep(random.randint(10,14))\n","    except BaseException as ex:\n","      if attempts < max_attempts:\n","        print('Unable to open url. Retrying in 30 seconds. \\\n","               See exception repr for details: ')\n","        print(repr(ex))      # print object as string\n","        time.sleep(30)\n","      else:\n","        print('Too many attepts, breaking loop')\n","        #log error to file.\n","        __errorLog(url, 'Too many attempts\\n' + repr(ex))\n","        break\n","    else:\n","      unsuccessful = False\n","      source = BeautifulSoup(driver.page_source, 'html.parser')\n","    \n","      # second, if we are interested in clicking a button,\n","      # we repeat the process using the existing driver to click the button\n","      if site == 'yelp' in url and SITES[site]['amenities_header_text'] in source.text:\n","        source = __getBS_yelp_button_press(driver, url, source, site)\n","\n","  driver.close()\n","  driver.quit()\n","  return source"],"metadata":{"id":"6XhkNMtbmZLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NOTE: this function works for yelp\n","\n","def get_search_result_links(url_curr, lst, site):\n","  '''\n","  # fills lst with a list of complete url's to each establishment's review site\n","  # methods in place to skip potential anchor tags.\n","  '''\n","  \n","  global SITES\n","  print('Gathering search result urls @ ' + url_curr)\n","  bs = getBS(url_curr)\n","  if bs:\n","    tags = bs.find_all(SITES[site]['search_results_tags'], SITES[site]['get_search_results_ext_tag_and_attribute'])\n","    for tag in tags:\n","      # check if advertisement is expected in search results based on SITES\n","      # and skip this anchor tag if it contains advertisement string in the href\n","      if 'ad_characters' in SITES[site].keys() and SITES[site]['ad_characters'] in tag.find('a')['href']:\n","        continue\n","      # if this tag is not an advertisement, add it to the list of urls\n","      else:\n","        # check if the href contains the full url or extension only\n","        url_ext = tag.find('a')['href']\n","        if 'remove_unwanted_characters_in_url' in SITES[site]:\n","          url_ext = re.sub(SITES[site]['remove_unwanted_characters_in_url'], '', url_ext)\n","        if SITES[site]['url'] in url_ext:\n","          lst.append(url_ext)\n","        else:\n","          lst.append(SITES[site]['url'] + url_ext)\n","\n","    # get the next page anchor on current page, then go to the next page recursively until there are no more pages.\n","    next_page = bs.find('a', SITES[site]['next_page_tag_attr_and_def'])\n","    if next_page != None:\n","      get_search_result_links(next_page['href'], lst, site)    \n","  else:\n","    print('Error getting BeautifulSoup object for ' + url_curr)\n","    __errorLog(url_curr, 'Error getting BeautifulSoup object in get_search_result_links')\n","\n","  # end method"],"metadata":{"id":"qQ0jgkR5O4d9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Collecting Methods\n","\n","These methods will be used by the user to access other methods above.\n","While methods above can still be used by the user, these are nicely packaged methods that will call methods sequentially to collect data from yelp, google, etc..\n","\n","---\n","Methods:\n","* collect_city_establishments_information\n","* collect_establishment_information\n","* get_search_result_url_list\n","---"],"metadata":{"id":"oAXDqvVA-0K5"}},{"cell_type":"code","source":["def collect_city_establishments_information(site, city = 'austin', state_abbr = 'TX'):\n","  '''\n","  # collects information \n","  '''\n","  global SITES, URL_LIST_FILENAMES\n","\n","  city = re.sub(' ', '_', city)\n","  state_abbr = state_abbr.upper()\n","\n","  # get three file names, one for brewery, one for distillery, and one for winery\n","  filenames = []\n","  for filename in URL_LIST_FILENAMES:\n","    if filename.startswith(city + '_' + state_abbr):\n","      filenames.append(filename)\n","\n","  url_list = []\n","  for filename in filenames:\n","    urls_from_file = load_url_list(filename)\n","    for url in urls_from_file:\n","      if url not in url_list:\n","        url_list.append(url)\n","\n","  csv_mat = []\n","  json_list = []\n","  # retrieves yelp establishment information\n","  if site == 'yelp':\n","    for url in url_list:\n","      # receive csvlst and jsondict for one establishment\n","      csv_list, json_dict = __get_yelp_est_info(site, url)\n","      csv_mat.append(csv_list)\n","\n","      # add json_dict for current establishment to the json_list\n","      json_list.append(json_dict)\n","\n","\n","  # end collecting, save json_list to json file.\n","  save_city_json(city + '_' + state_abbr + '.json', json_list)\n","\n","  # end collecting, save csv_mat to csv file\n","  save_city_csv(city + '_' + state_abbr + '.json', csv_mat)"],"metadata":{"id":"fs_u0Ei_8viH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collect_establishment_information(site, link):\n","  '''\n","  # returns 'infoCSV' and 'infoJSON' with data for the chosen 'site' being scraped\n","  # this method is called once for each establishment\n","  # NOTE: this function works for yelp\n","  # is this function worth keeping?  for one-offs perhaps?\n","  '''\n","  print('Attempting to collect from ' + link)\n","  \n","  # retrieves yelp establishment information\n","  if site == 'yelp':  \n","    return __get_yelp_est_info(site, link)"],"metadata":{"id":"3rSQP_K_B3PD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_search_result_url_list(site = 'yelp', city = 'austin', state_abbr = 'TX', est_type = 'distilleries'):\n","  '''\n","  # returns a list of urls for all establishments in a search result based on SITES dictionary\n","  '''\n","  global SITES, ROOT\n","  url_list = []\n","  link = None\n","  if site == 'yelp':\n","    state_abbr = state_abbr.upper()\n","    city = re.sub('[ _]', '+', city) # replace spaces with plus signs for url text\n","    link = 'https://www.yelp.com/search?find_desc=' + est_type + '&find_loc=' + city + '%2C+' + state_abbr\n","  get_search_result_links(link, url_list, site)    # this function is recursively called and returns nothing but populates url_list\n","  city = re.sub('[+]', '_', city) # replace plus sign with underscore for filenaming\n","  save_url_list(city + '_' + state_abbr + '_' + est_type +'.txt', url_list)\n","  return url_list"],"metadata":{"id":"vCFPfo8EUnIA"},"execution_count":null,"outputs":[]}]}